{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TC 5033\n",
    "## Deep Learning\n",
    "## Transformers\n",
    "\n",
    "#### Activity 4: Implementing a Translator\n",
    "\n",
    "- Objective\n",
    "\n",
    "To understand the Transformer Architecture by Implementing a translator.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams. While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Follow the provided code. The code already implements a transformer from scratch as explained in one of [week's 9 videos](https://youtu.be/XefFj4rLHgU)\n",
    "\n",
    "    Since the provided code already implements a simple translator, your job for this assignment is to understand it fully, and document it using pictures, figures, and markdown cells.  You should test your translator with at least 10 sentences. The dataset used for this task was obtained from [Tatoeba, a large dataset of sentences and translations](https://tatoeba.org/en/downloads).\n",
    "  \n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Traning a translator\n",
    "    - Translating at least 10 sentences.\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install packages\n",
    "# %pip install -q torch --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install -q torcheval\n",
    "# %pip install -q tatoebatools numpy pandas dask[dataframe]\n",
    "# %pip install -q torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.hub\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "import torcheval.metrics.functional as metrics\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from collections import Counter\n",
    "from tatoebatools import tatoeba\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from utils import get_device, DummyTokenizer, Truncate, AddToken, VocabTransform, ToTensor, PadTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = get_device()\n",
    "\n",
    "TXT_DIR = './translations'\n",
    "TXT_SRC = 'eng'\n",
    "TXT_TGT = 'spa'\n",
    "\n",
    "TOK_SRC = 'en'\n",
    "TOK_TGT = 'es'\n",
    "\n",
    "MAX_LEN = 30\n",
    "BATCH_SIZE = 96\n",
    "MODEL_DIM = 512\n",
    "NUM_HEADS = 8\n",
    "HIDDEN_DIM = 2024\n",
    "ENC_NUM_LAYERS = 6\n",
    "DEC_NUM_LAYERS = 6\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 20\n",
    "\n",
    "UNK_TOK = '[unk]'\n",
    "PAD_TOK = '[pad]'\n",
    "BOS_TOK = '[bos]'\n",
    "EOS_TOK = '[eos]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available device: {DEVICE.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataset using tatoebatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tatoeba.dir = TXT_DIR\n",
    "\n",
    "src_sentences_df = tatoeba.get('sentences_detailed', [TXT_SRC])\n",
    "tgt_sentences_df = tatoeba.get('sentences_detailed', [TXT_TGT])\n",
    "src_tgt_links_df = tatoeba.get('links', language_codes=[TXT_SRC, TXT_TGT])\n",
    "\n",
    "user_df = tatoeba.get('user_languages', language_codes='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair spanish and english sentences correctly into a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translations(src_df, tgt_df, links_df, user_df, level=0.0):\n",
    "    df1 = pd.merge(src_df, links_df, how='left', left_on='sentence_id', right_on='sentence_id')\n",
    "    df2 = pd.merge(df1, tgt_df, how='left', left_on='translation_id', right_on='sentence_id')\n",
    "    df3 = pd.merge(df2, user_df, how='left', left_on='username_y', right_on='username')\n",
    "\n",
    "    filter = (df3.skill_level >= level) & (df3.username != np.nan)\n",
    "    \n",
    "    return df3.where(filter).dropna()[['sentence_id_x', 'lang_x', 'text_x', 'sentence_id_y', 'lang_y', 'text_y']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96139"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(TXT_DIR, \"translations.csv\")):\n",
    "    translations_df = get_translations(src_sentences_df, tgt_sentences_df, src_tgt_links_df, user_df)\n",
    "    translations_df = translations_df.drop_duplicates(subset=['text_x', 'text_y']).reset_index(drop=True)\n",
    "    translations_df.to_csv(os.path.join(TXT_DIR, \"translations.csv\"))\n",
    "else:\n",
    "    translations_df = pd.read_csv(os.path.join(TXT_DIR, \"translations.csv\"))\n",
    "    \n",
    "len(translations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_id_x</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>text_x</th>\n",
       "      <th>sentence_id_y</th>\n",
       "      <th>lang_y</th>\n",
       "      <th>text_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27269</th>\n",
       "      <td>27269</td>\n",
       "      <td>2236266.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>Tom felt safe.</td>\n",
       "      <td>9432153.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Tom se sintió seguro.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9474</th>\n",
       "      <td>9474</td>\n",
       "      <td>978516.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>Tom doesn't want to live in Boston for more th...</td>\n",
       "      <td>978520.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Tom no quiere vivir en Boston por más de un año.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65944</th>\n",
       "      <td>65944</td>\n",
       "      <td>7782709.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>The text is illegible.</td>\n",
       "      <td>7551085.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>El texto es ilegible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92994</th>\n",
       "      <td>92994</td>\n",
       "      <td>11537073.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>We lost our passports on the trip.</td>\n",
       "      <td>11537116.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Perdimos nuestros pasaportes en el viaje.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33999</th>\n",
       "      <td>33999</td>\n",
       "      <td>2431559.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>I'm not very good at making pizza, but Tom is.</td>\n",
       "      <td>2486055.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>No se me da muy bien hacer pizzas, pero a Tom sí.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29096</th>\n",
       "      <td>29096</td>\n",
       "      <td>2253183.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>I haven't formed an opinion on that subject yet.</td>\n",
       "      <td>2253184.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Todavía no me he formado una opinión sobre ese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52343</th>\n",
       "      <td>52343</td>\n",
       "      <td>4771575.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>Tom doesn't care what I do.</td>\n",
       "      <td>4725490.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>A Tom le da igual lo que yo haga.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93972</th>\n",
       "      <td>93972</td>\n",
       "      <td>11830071.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>Ivan’s dog was terrified of something.</td>\n",
       "      <td>11830090.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Al perro de Iván le aterrorizaba algo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76180</th>\n",
       "      <td>76180</td>\n",
       "      <td>9432352.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>It's on the other side of the river.</td>\n",
       "      <td>11836154.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Está al otro lado del río.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16276</th>\n",
       "      <td>16276</td>\n",
       "      <td>1553395.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>Something's going to happen. I can feel it.</td>\n",
       "      <td>1613658.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Va a pasar algo. Puedo sentirlo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87225</th>\n",
       "      <td>87225</td>\n",
       "      <td>10555392.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>She has chronic bronchitis.</td>\n",
       "      <td>10963777.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Tiene bronquitis crónica.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>2083</td>\n",
       "      <td>449019.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>It's nothing serious.</td>\n",
       "      <td>806218.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>No es nada grave.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31539</th>\n",
       "      <td>31539</td>\n",
       "      <td>2268930.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>This is a mechanical toy.</td>\n",
       "      <td>2268931.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Este juguete es mecánico.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>4921</td>\n",
       "      <td>645080.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>You must read this book.</td>\n",
       "      <td>645714.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Tienes que leerte este libro.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14490</th>\n",
       "      <td>14490</td>\n",
       "      <td>1349222.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>I have nothing.</td>\n",
       "      <td>7591212.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Nada tengo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61063</th>\n",
       "      <td>61063</td>\n",
       "      <td>6681961.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>They like to be alone.</td>\n",
       "      <td>6681516.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Les gusta estar solos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22938</th>\n",
       "      <td>22938</td>\n",
       "      <td>2007428.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>Let's talk about basketball.</td>\n",
       "      <td>4471264.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Hablemos de basquetbol.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91063</th>\n",
       "      <td>91063</td>\n",
       "      <td>11162415.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>You are not ugly.</td>\n",
       "      <td>11162476.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>No son feos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85313</th>\n",
       "      <td>85313</td>\n",
       "      <td>10317584.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>Tomorrow is my thirtieth birthday.</td>\n",
       "      <td>10317626.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Mañana es mi trigésimo cumpleaños.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16322</th>\n",
       "      <td>16322</td>\n",
       "      <td>1553463.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>I know what happened to Tom.</td>\n",
       "      <td>1748294.0</td>\n",
       "      <td>spa</td>\n",
       "      <td>Sé lo que le pasó a Tom.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  sentence_id_x lang_x  \\\n",
       "27269       27269      2236266.0    eng   \n",
       "9474         9474       978516.0    eng   \n",
       "65944       65944      7782709.0    eng   \n",
       "92994       92994     11537073.0    eng   \n",
       "33999       33999      2431559.0    eng   \n",
       "29096       29096      2253183.0    eng   \n",
       "52343       52343      4771575.0    eng   \n",
       "93972       93972     11830071.0    eng   \n",
       "76180       76180      9432352.0    eng   \n",
       "16276       16276      1553395.0    eng   \n",
       "87225       87225     10555392.0    eng   \n",
       "2083         2083       449019.0    eng   \n",
       "31539       31539      2268930.0    eng   \n",
       "4921         4921       645080.0    eng   \n",
       "14490       14490      1349222.0    eng   \n",
       "61063       61063      6681961.0    eng   \n",
       "22938       22938      2007428.0    eng   \n",
       "91063       91063     11162415.0    eng   \n",
       "85313       85313     10317584.0    eng   \n",
       "16322       16322      1553463.0    eng   \n",
       "\n",
       "                                                  text_x  sentence_id_y  \\\n",
       "27269                                     Tom felt safe.      9432153.0   \n",
       "9474   Tom doesn't want to live in Boston for more th...       978520.0   \n",
       "65944                             The text is illegible.      7551085.0   \n",
       "92994                 We lost our passports on the trip.     11537116.0   \n",
       "33999     I'm not very good at making pizza, but Tom is.      2486055.0   \n",
       "29096   I haven't formed an opinion on that subject yet.      2253184.0   \n",
       "52343                        Tom doesn't care what I do.      4725490.0   \n",
       "93972             Ivan’s dog was terrified of something.     11830090.0   \n",
       "76180               It's on the other side of the river.     11836154.0   \n",
       "16276        Something's going to happen. I can feel it.      1613658.0   \n",
       "87225                        She has chronic bronchitis.     10963777.0   \n",
       "2083                               It's nothing serious.       806218.0   \n",
       "31539                          This is a mechanical toy.      2268931.0   \n",
       "4921                            You must read this book.       645714.0   \n",
       "14490                                    I have nothing.      7591212.0   \n",
       "61063                             They like to be alone.      6681516.0   \n",
       "22938                       Let's talk about basketball.      4471264.0   \n",
       "91063                                  You are not ugly.     11162476.0   \n",
       "85313                 Tomorrow is my thirtieth birthday.     10317626.0   \n",
       "16322                       I know what happened to Tom.      1748294.0   \n",
       "\n",
       "      lang_y                                             text_y  \n",
       "27269    spa                              Tom se sintió seguro.  \n",
       "9474     spa   Tom no quiere vivir en Boston por más de un año.  \n",
       "65944    spa                              El texto es ilegible.  \n",
       "92994    spa          Perdimos nuestros pasaportes en el viaje.  \n",
       "33999    spa  No se me da muy bien hacer pizzas, pero a Tom sí.  \n",
       "29096    spa  Todavía no me he formado una opinión sobre ese...  \n",
       "52343    spa                  A Tom le da igual lo que yo haga.  \n",
       "93972    spa             Al perro de Iván le aterrorizaba algo.  \n",
       "76180    spa                         Está al otro lado del río.  \n",
       "16276    spa                   Va a pasar algo. Puedo sentirlo.  \n",
       "87225    spa                          Tiene bronquitis crónica.  \n",
       "2083     spa                                  No es nada grave.  \n",
       "31539    spa                          Este juguete es mecánico.  \n",
       "4921     spa                      Tienes que leerte este libro.  \n",
       "14490    spa                                        Nada tengo.  \n",
       "61063    spa                             Les gusta estar solos.  \n",
       "22938    spa                            Hablemos de basquetbol.  \n",
       "91063    spa                                       No son feos.  \n",
       "85313    spa                 Mañana es mi trigésimo cumpleaños.  \n",
       "16322    spa                           Sé lo que le pasó a Tom.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations_df.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vocabulary for both source and target languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rownames = {\n",
    "    TXT_SRC: 'text_x',\n",
    "    TXT_TGT: 'text_y',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentece(sentence: str):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"[á]+\", \"a\", sentence)\n",
    "    sentence = re.sub(r\"[é]+\", \"e\", sentence)\n",
    "    sentence = re.sub(r\"[í]+\", \"i\", sentence)\n",
    "    sentence = re.sub(r\"[ó]+\", \"o\", sentence)\n",
    "    sentence = re.sub(r\"[ú]+\", \"u\", sentence)\n",
    "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    return sentence.strip().split()\n",
    "\n",
    "def idx2word(word_count: Counter, specials = [PAD_TOK, UNK_TOK, BOS_TOK, EOS_TOK]):\n",
    "    sorted_word_count = sorted(word_count.items())\n",
    "    specials = { token: idx for idx, token in enumerate(specials) }\n",
    "    word2idx = { word: idx for idx, (word, _) in enumerate(sorted_word_count, start=len(specials)) }\n",
    "    \n",
    "    word2idx = { **specials, **word2idx }\n",
    "    idx2word = { idx: word for word, idx in word2idx.items() }\n",
    "    \n",
    "    return word2idx, idx2word    \n",
    "\n",
    "\n",
    "def build_vocab(dataframe, rownames):\n",
    "    c1, c2 = Counter(), Counter()\n",
    "    ddf = dd.from_pandas(dataframe, npartitions=multiprocessing.cpu_count())\n",
    "    \n",
    "    def process(row):\n",
    "        tokens = preprocess_sentece(row[rownames[TXT_SRC]])\n",
    "        c1.update(tokens)\n",
    "        \n",
    "        tokens = preprocess_sentece(row[rownames[TXT_TGT]])\n",
    "        c2.update(tokens)\n",
    "        \n",
    "    ddf.apply(process, axis=1, meta=dataframe).compute()\n",
    "    \n",
    "    return c1, c2\n",
    "\n",
    "\n",
    "src, tgt = build_vocab(translations_df, rownames)\n",
    "\n",
    "src_word2idx, src_idx2word = idx2word(src)\n",
    "tgt_word2idx, tgt_idx2word = idx2word(tgt)\n",
    "\n",
    "src_vocab_len = len(src_word2idx)\n",
    "tgt_vocab_len = len(tgt_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencesDataset(utils.Dataset):\n",
    "    \n",
    "    def __init__(self, translations_df: pd.DataFrame, src_word2idx: dict, tgt_word2idx: dict):\n",
    "        self.translations_df = translations_df\n",
    "        self.src_word2idx = src_word2idx\n",
    "        self.tgt_word2idx = tgt_word2idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.translations_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.translations_df.iloc[index]\n",
    "        return row.text_x, row.text_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations_dataset = SentencesDataset(translations_df, src_word2idx, tgt_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76912, 14421, 4806)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, valid_dataset, test_dataset = utils.random_split(translations_dataset, lengths=[0.8, 0.15, 0.05])\n",
    "\n",
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_transforms = nn.Sequential(\n",
    "    DummyTokenizer(tokenizer_fn=preprocess_sentece),\n",
    "    Truncate(max_seq_len=MAX_LEN-2),\n",
    "    AddToken(token_id=BOS_TOK, begin=True),\n",
    "    AddToken(token_id=EOS_TOK, begin=False),\n",
    "    VocabTransform(word2idx=src_word2idx, unk_tok=src_word2idx.get(UNK_TOK)),\n",
    "    ToTensor(padding_value=src_word2idx.get(PAD_TOK), dtype=torch.long),\n",
    "    PadTransform(max_length=MAX_LEN, pad_value=src_word2idx.get(PAD_TOK))\n",
    ")\n",
    "\n",
    "tgt_transforms = nn.Sequential(\n",
    "    DummyTokenizer(tokenizer_fn=preprocess_sentece),\n",
    "    Truncate(max_seq_len=MAX_LEN-2),\n",
    "    AddToken(token_id=BOS_TOK, begin=True),\n",
    "    VocabTransform(word2idx=tgt_word2idx, unk_tok=tgt_word2idx.get(UNK_TOK)),\n",
    "    ToTensor(padding_value=tgt_word2idx.get(PAD_TOK), dtype=torch.long),\n",
    "    PadTransform(max_length=MAX_LEN, pad_value=tgt_word2idx.get(PAD_TOK))\n",
    ")\n",
    "\n",
    "label_transforms = nn.Sequential(\n",
    "    DummyTokenizer(tokenizer_fn=preprocess_sentece),\n",
    "    Truncate(max_seq_len=MAX_LEN-2),\n",
    "    AddToken(token_id=EOS_TOK, begin=False),\n",
    "    VocabTransform(word2idx=tgt_word2idx, unk_tok=tgt_word2idx.get(UNK_TOK)),\n",
    "    ToTensor(padding_value=tgt_word2idx.get(PAD_TOK), dtype=torch.long),\n",
    "    PadTransform(max_length=MAX_LEN, pad_value=tgt_word2idx.get(PAD_TOK))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_fn(batch):\n",
    "    x, y = list(zip(*batch))\n",
    "    return src_transforms(x), tgt_transforms(y), label_transforms(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = utils.DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch_fn, shuffle=True)\n",
    "valid_dataloader = utils.DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch_fn, shuffle=True)\n",
    "test_dataloader = utils.DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 30]) tensor([    2,  8093, 12665, 16552, 10721, 16767, 16552, 11320,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "torch.Size([96, 30]) tensor([    2, 30198, 23049, 17452, 19672,  2768,  1250, 18676,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "torch.Size([96, 30]) tensor([30198, 23049, 17452, 19672,  2768,  1250, 18676,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "for x, y, label in train_dataloader:\n",
    "    print(x.shape, x[0])\n",
    "    print(y.shape, y[0])\n",
    "    print(label.shape, label[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Seq2SeqTranslatorTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "Seq2SeqTranslatorTransformer                  [96, 30, 30311]           --\n",
       "├─Embedding: 1-1                              [96, 30, 512]             9,500,160\n",
       "├─PositionalEncoding: 1-2                     [96, 30, 512]             --\n",
       "│    └─Dropout: 2-1                           [96, 30, 512]             --\n",
       "├─Encoder: 1-3                                [96, 30, 512]             --\n",
       "│    └─ModuleList: 2-2                        --                        --\n",
       "│    │    └─EncoderBlock: 3-1                 [96, 30, 512]             3,125,736\n",
       "│    │    └─EncoderBlock: 3-2                 [96, 30, 512]             3,125,736\n",
       "│    │    └─EncoderBlock: 3-3                 [96, 30, 512]             3,125,736\n",
       "│    │    └─EncoderBlock: 3-4                 [96, 30, 512]             3,125,736\n",
       "│    │    └─EncoderBlock: 3-5                 [96, 30, 512]             3,125,736\n",
       "│    │    └─EncoderBlock: 3-6                 [96, 30, 512]             3,125,736\n",
       "│    └─LayerNorm: 2-3                         [96, 30, 512]             1,024\n",
       "├─Embedding: 1-4                              [96, 30, 512]             15,519,232\n",
       "├─PositionalEncoding: 1-5                     [96, 30, 512]             --\n",
       "│    └─Dropout: 2-4                           [96, 30, 512]             --\n",
       "├─Decoder: 1-6                                [96, 30, 512]             --\n",
       "│    └─ModuleList: 2-5                        --                        --\n",
       "│    │    └─DecoderBlock: 3-7                 [96, 30, 512]             4,175,336\n",
       "│    │    └─DecoderBlock: 3-8                 [96, 30, 512]             4,175,336\n",
       "│    │    └─DecoderBlock: 3-9                 [96, 30, 512]             4,175,336\n",
       "│    │    └─DecoderBlock: 3-10                [96, 30, 512]             4,175,336\n",
       "│    │    └─DecoderBlock: 3-11                [96, 30, 512]             4,175,336\n",
       "│    │    └─DecoderBlock: 3-12                [96, 30, 512]             4,175,336\n",
       "│    └─LayerNorm: 2-6                         [96, 30, 512]             1,024\n",
       "├─Linear: 1-7                                 [96, 30, 30311]           15,549,543\n",
       "===============================================================================================\n",
       "Total params: 84,377,415\n",
       "Trainable params: 84,377,415\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 8.10\n",
       "===============================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 2649.95\n",
       "Params size (MB): 337.51\n",
       "Estimated Total Size (MB): 2987.50\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = Seq2SeqTranslatorTransformer(\n",
    "    dim_model=MODEL_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    max_len=MAX_LEN,\n",
    "    enc_num_layers=ENC_NUM_LAYERS,\n",
    "    dec_num_layers=DEC_NUM_LAYERS,\n",
    "    src_vocab_size=src_vocab_len,\n",
    "    tgt_vocab_size=tgt_vocab_len,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "x, y, label = next(iter(train_dataloader))\n",
    "\n",
    "summary(translator, input_data=[x, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of train and test step functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, dataloader):\n",
    "    total_loss = torch.zeros(len(dataloader))\n",
    "    \n",
    "    model.train()\n",
    "    for i, (enc_input, dec_input, label) in enumerate(tqdm(dataloader)):\n",
    "        enc_input, dec_input, label = enc_input.to(DEVICE), dec_input.to(DEVICE), label.to(DEVICE)\n",
    "        \n",
    "        # compute forward pass\n",
    "        logits = model(enc_input, dec_input)\n",
    "        \n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        label = label.contiguous().view(-1)\n",
    "\n",
    "        # compute loss, gradients, and update params\n",
    "        loss = F.cross_entropy(\n",
    "            logits, label, ignore_index=tgt_word2idx.get(PAD_TOK))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # update metrics\n",
    "        total_loss[i] = loss.item()\n",
    "        \n",
    "          \n",
    "    # Compute avg\n",
    "    avg_loss = total_loss.mean()\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader):\n",
    "    total_loss = torch.zeros(len(dataloader))\n",
    "    \n",
    "    model.eval()\n",
    "    for i, (enc_input, dec_input, label) in enumerate(dataloader):\n",
    "        enc_input, dec_input, label = enc_input.to(DEVICE), dec_input.to(DEVICE), label.to(DEVICE)\n",
    "                \n",
    "        # compute forward pass\n",
    "        logits = model(enc_input, dec_input)\n",
    "\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        label = label.contiguous().view(-1)\n",
    "       \n",
    "        # compute loss\n",
    "        loss = F.cross_entropy(logits, label, ignore_index=tgt_word2idx.get(PAD_TOK))\n",
    "        \n",
    "        # update metrics\n",
    "        total_loss[i] = loss.item()\n",
    "\n",
    "    # Compute avg\n",
    "    avg_loss = total_loss.mean()\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_dataloader, test_dataloader, epochs=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch={i+1}\")\n",
    "        train_avg_loss = train_epoch(model, optimizer, train_dataloader)\n",
    "        valid_avg_loss = validate_epoch(model, test_dataloader)\n",
    "        print(f\"Train Loss={train_avg_loss:>7f} \\t Valid Loss={valid_avg_loss:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Seq2SeqTranslatorTransformer(\n",
    "    dim_model=MODEL_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    max_len=MAX_LEN,\n",
    "    enc_num_layers=ENC_NUM_LAYERS,\n",
    "    dec_num_layers=DEC_NUM_LAYERS,\n",
    "    src_vocab_size=src_vocab_len,\n",
    "    tgt_vocab_size=tgt_vocab_len,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=6.354481 \t Valid Loss=5.925820\n",
      "Epoch=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=5.534527 \t Valid Loss=5.197404\n",
      "Epoch=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.942280 \t Valid Loss=4.773385\n",
      "Epoch=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.499310 \t Valid Loss=4.363037\n",
      "Epoch=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=4.118775 \t Valid Loss=4.063642\n",
      "Epoch=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.789316 \t Valid Loss=3.805299\n",
      "Epoch=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.498801 \t Valid Loss=3.567425\n",
      "Epoch=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.248730 \t Valid Loss=3.386536\n",
      "Epoch=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=3.018673 \t Valid Loss=3.231917\n",
      "Epoch=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=2.809431 \t Valid Loss=3.109604\n",
      "Epoch=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=2.618526 \t Valid Loss=2.985568\n",
      "Epoch=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=2.439417 \t Valid Loss=2.891711\n",
      "Epoch=13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=2.276958 \t Valid Loss=2.827249\n",
      "Epoch=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=2.124901 \t Valid Loss=2.737869\n",
      "Epoch=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=1.984834 \t Valid Loss=2.679720\n",
      "Epoch=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=1.852399 \t Valid Loss=2.627661\n",
      "Epoch=17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=1.726209 \t Valid Loss=2.591347\n",
      "Epoch=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=1.607944 \t Valid Loss=2.561717\n",
      "Epoch=19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=1.497664 \t Valid Loss=2.537451\n",
      "Epoch=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:39<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss=1.394727 \t Valid Loss=2.536337\n"
     ]
    }
   ],
   "source": [
    "train(translator, train_dataloader, valid_dataloader, epochs=EPOCHS)\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "path = f\"./transformer_{timestamp}.pkl\"\n",
    "\n",
    "torch.save(translator.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate sentences using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_sentence(indices, idx2word: dict):\n",
    "    return \" \".join([idx2word.get(idx) for idx in indices if idx2word.get(idx) != PAD_TOK])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, src, max_seq_len=MAX_LEN):\n",
    "    src_idx = src_transforms([src])\n",
    "    \n",
    "    tgt_idx = torch.zeros(1, 1, dtype=torch.long)\n",
    "    tgt_idx[0, 0] = tgt_word2idx.get(BOS_TOK)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_seq_len):\n",
    "            # crop the last max seq len indices\n",
    "            tgt_idx = tgt_idx[:, -max_seq_len:]\n",
    "\n",
    "            # evaluate the model\n",
    "            logits = model(src_idx.to(DEVICE), tgt_idx.to(DEVICE))            \n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # get the next token                        \n",
    "            _, next_token = torch.max(probs, dim=-1, keepdim=True)\n",
    "\n",
    "            if next_token.cpu().item() == tgt_word2idx.get(EOS_TOK):\n",
    "                break\n",
    "\n",
    "            tgt_idx = torch.cat([tgt_idx, next_token.cpu()], dim=1)\n",
    "\n",
    "    return tgt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentences(model, dataset, max_seq_len=MAX_LEN):\n",
    "    model.eval()\n",
    "\n",
    "    for i, (src, tgt) in enumerate(dataset):\n",
    "        sequence = translate_sentence(model, src, max_seq_len)\n",
    "        translation = indices_to_sentence(sequence[0].numpy(), tgt_idx2word)\n",
    "        \n",
    "        if i >= 10: break\n",
    "        yield src, tgt, translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: Ask for it.\n",
      "correct: Pedidlo.\n",
      "translation: [bos] preguntale por ello\n",
      "\n",
      "\n",
      "sentence: I knew there was nothing you could do about it.\n",
      "correct: Yo sabía que no había nada que pudieras hacer al respecto.\n",
      "translation: [bos] supe que no habia nada que pudieras hacer por ello\n",
      "\n",
      "\n",
      "sentence: I wish I could be with you.\n",
      "correct: Ojalá pudiera estar con vosotras.\n",
      "translation: [bos] me gustaria poder estar contigo\n",
      "\n",
      "\n",
      "sentence: Who's the girl with you?\n",
      "correct: ¿Quién es la chica que está contigo?\n",
      "translation: [bos] quien es la chica con usted\n",
      "\n",
      "\n",
      "sentence: You can count on me to be there by 10:00.\n",
      "correct: Estaré allí sobre las 10:00.\n",
      "translation: [bos] puede contar conmigo\n",
      "\n",
      "\n",
      "sentence: She'll know.\n",
      "correct: Se va a saber.\n",
      "translation: [bos] ella lo sabra\n",
      "\n",
      "\n",
      "sentence: I'll tell everything to my father and mother.\n",
      "correct: Se lo diré todo a mi padre y a mi madre.\n",
      "translation: [bos] se lo contare todo a mi padre y a mi madre\n",
      "\n",
      "\n",
      "sentence: I have a wonderful brother-in-law and sister-in-law. But why do you envy me?\n",
      "correct: Tengo un cuñado y una cuñada maravillosos. Pero ¿por qué me envidias?\n",
      "translation: [bos] tengo un hermano mayor en la cocina y mi hermana pero por mi me hace un poco\n",
      "\n",
      "\n",
      "sentence: Whatcha making?\n",
      "correct: ¿Qué 'tás haciendo?\n",
      "translation: [bos] tranquilo ocurren\n",
      "\n",
      "\n",
      "sentence: Get out of my car.\n",
      "correct: Sal de mi coche.\n",
      "translation: [bos] fuera de mi coche\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence, correct, translation in evaluate_sentences(translator, test_dataset):\n",
    "    print(f\"sentence: {sentence}\\ncorrect: {correct}\\ntranslation: {translation}\", end=\"\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
